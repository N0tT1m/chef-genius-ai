#!/usr/bin/env python3
"""
Dual Tokenizer Manager for Chef Genius
Manages both Enterprise B2B and Consumer End User tokenizers
Provides unified interface and training coordination
"""

import os
import sys
import json
import time
import requests
from pathlib import Path
from typing import Dict, List, Any, Optional, Union
from dataclasses import dataclass
import concurrent.futures
from transformers import PreTrainedTokenizerFast

# Import our custom tokenizers
from enterprise_b2b_tokenizer import EnterpriseB2BTokenizer
from consumer_end_user_tokenizer import ConsumerEndUserTokenizer

@dataclass
class TokenizerConfig:
    """Configuration for tokenizer training."""
    name: str
    vocab_size: int
    output_dir: str
    corpus_file: str
    target_domain: str
    description: str

class DualTokenizerManager:
    """
    Manages both enterprise B2B and consumer end user tokenizers.
    Provides unified training, testing, and deployment interface.
    """
    
    def __init__(self, discord_webhook: str = None):
        self.discord_webhook = discord_webhook
        self.tokenizers = {}
        self.configs = {}
        
        # Initialize tokenizer configurations
        self.tokenizer_configs = {
            "enterprise_b2b": TokenizerConfig(
                name="Enterprise B2B",
                vocab_size=50000,
                output_dir="enterprise_b2b_tokenizer",
                corpus_file="enterprise_b2b_corpus.txt",
                target_domain="commercial_food_service",
                description="Commercial kitchens, large batch cooking, food safety compliance"
            ),
            "consumer_end_user": TokenizerConfig(
                name="Consumer End User",
                vocab_size=32000,
                output_dir="consumer_end_user_tokenizer",
                corpus_file="consumer_end_user_corpus.txt",
                target_domain="home_cooking_consumer",
                description="Home kitchens, family recipes, casual cooking"
            )
        }
    
    def train_all_tokenizers(self, parallel: bool = True) -> Dict[str, Any]:
        """Train both tokenizers simultaneously or sequentially."""
        
        print("üî• DUAL TOKENIZER TRAINING PIPELINE")
        print("Training both Enterprise B2B and Consumer End User tokenizers")
        print("=" * 80)
        
        start_time = time.time()
        results = {}
        
        if parallel:
            print("‚ö° Training tokenizers in parallel for maximum speed...")
            results = self._train_parallel()
        else:
            print("üîÑ Training tokenizers sequentially...")
            results = self._train_sequential()
        
        total_time = time.time() - start_time
        
        # Compile final results
        final_results = {
            "training_mode": "parallel" if parallel else "sequential",
            "total_training_time": total_time,
            "tokenizers_trained": len(results),
            "results": results,
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
        }
        
        # Send summary notification
        self.send_training_summary_notification(final_results)
        
        return final_results
    
    def _train_parallel(self) -> Dict[str, Any]:
        """Train both tokenizers in parallel using ThreadPoolExecutor."""
        
        results = {}
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
            # Submit both training jobs
            future_to_tokenizer = {
                executor.submit(self._train_enterprise_b2b): "enterprise_b2b",
                executor.submit(self._train_consumer_end_user): "consumer_end_user"
            }
            
            # Collect results as they complete
            for future in concurrent.futures.as_completed(future_to_tokenizer):
                tokenizer_type = future_to_tokenizer[future]
                try:
                    result = future.result()
                    results[tokenizer_type] = result
                    print(f"‚úÖ {tokenizer_type} tokenizer training completed!")
                except Exception as e:
                    print(f"‚ùå {tokenizer_type} tokenizer training failed: {e}")
                    results[tokenizer_type] = {"error": str(e)}
        
        return results
    
    def _train_sequential(self) -> Dict[str, Any]:
        """Train tokenizers one after another."""
        
        results = {}
        
        # Train Enterprise B2B first
        try:
            print("\nüè≠ Training Enterprise B2B Tokenizer...")
            results["enterprise_b2b"] = self._train_enterprise_b2b()
            print("‚úÖ Enterprise B2B tokenizer training completed!")
        except Exception as e:
            print(f"‚ùå Enterprise B2B tokenizer training failed: {e}")
            results["enterprise_b2b"] = {"error": str(e)}
        
        # Train Consumer End User second
        try:
            print("\nüè† Training Consumer End User Tokenizer...")
            results["consumer_end_user"] = self._train_consumer_end_user()
            print("‚úÖ Consumer End User tokenizer training completed!")
        except Exception as e:
            print(f"‚ùå Consumer End User tokenizer training failed: {e}")
            results["consumer_end_user"] = {"error": str(e)}
        
        return results
    
    def _train_enterprise_b2b(self) -> Dict[str, Any]:
        """Train the enterprise B2B tokenizer."""
        
        config = self.tokenizer_configs["enterprise_b2b"]
        
        # Create tokenizer trainer
        trainer = EnterpriseB2BTokenizer(discord_webhook=self.discord_webhook)
        trainer.vocab_size = config.vocab_size
        
        # Create corpus
        corpus_file = trainer.create_training_corpus(config.corpus_file)
        
        # Train tokenizer
        trained_tokenizer = trainer.train_tokenizer(corpus_file)
        
        # Test tokenizer
        test_results = trainer.test_tokenizer(trained_tokenizer)
        
        # Save tokenizer
        saved_path = trainer.save_tokenizer(trained_tokenizer, config.output_dir)
        
        # Store tokenizer for later use
        self.tokenizers["enterprise_b2b"] = trained_tokenizer
        self.configs["enterprise_b2b"] = config
        
        return {
            "config": config,
            "corpus_file": corpus_file,
            "saved_path": saved_path,
            "test_results": test_results,
            "vocab_size": len(trained_tokenizer.get_vocab()),
            "training_success": True
        }
    
    def _train_consumer_end_user(self) -> Dict[str, Any]:
        """Train the consumer end user tokenizer."""
        
        config = self.tokenizer_configs["consumer_end_user"]
        
        # Create tokenizer trainer
        trainer = ConsumerEndUserTokenizer(discord_webhook=self.discord_webhook)
        trainer.vocab_size = config.vocab_size
        
        # Create corpus
        corpus_file = trainer.create_training_corpus(config.corpus_file)
        
        # Train tokenizer
        trained_tokenizer = trainer.train_tokenizer(corpus_file)
        
        # Test tokenizer
        test_results = trainer.test_tokenizer(trained_tokenizer)
        
        # Save tokenizer
        saved_path = trainer.save_tokenizer(trained_tokenizer, config.output_dir)
        
        # Store tokenizer for later use
        self.tokenizers["consumer_end_user"] = trained_tokenizer
        self.configs["consumer_end_user"] = config
        
        return {
            "config": config,
            "corpus_file": corpus_file,
            "saved_path": saved_path,
            "test_results": test_results,
            "vocab_size": len(trained_tokenizer.get_vocab()),
            "training_success": True
        }
    
    def compare_tokenizers(self, test_texts: List[str] = None) -> Dict[str, Any]:
        """Compare both tokenizers on the same test texts."""
        
        if not test_texts:
            test_texts = [
                # Mixed domain examples
                "Prepare 100 servings of chicken stir-fry using the combi_oven at 350¬∞F for food service.",
                "Make this easy 30-minute weeknight dinner that's kid-friendly and budget-friendly.",
                "Implement HACCP protocol for temperature control in the walk-in cooler during service.",
                "This comfort food recipe brings restaurant quality to your home kitchen.",
                "Calculate recipe_multiplier for scaling from 50 to 200 portions while maintaining food_cost_percentage.",
                "Weekend baking project: homemade sourdough bread perfect for family breakfast.",
                "Monitor blast_chiller temperature logs for regulatory compliance and food safety.",
                "Quick meal prep ideas for busy parents using pantry staples and frozen vegetables."
            ]
        
        print("üîç TOKENIZER COMPARISON ANALYSIS")
        print(f"Testing {len(test_texts)} examples across both tokenizers")
        print("=" * 80)
        
        if not self.tokenizers:
            print("‚ö†Ô∏è No tokenizers loaded. Please train tokenizers first.")
            return {}
        
        comparison_results = []
        
        for i, text in enumerate(test_texts, 1):
            print(f"\nTest {i}: {text[:60]}...")
            
            result = {
                "text": text,
                "enterprise_b2b": {},
                "consumer_end_user": {},
                "comparison": {}
            }
            
            # Test with Enterprise B2B tokenizer
            if "enterprise_b2b" in self.tokenizers:
                b2b_tokenizer = self.tokenizers["enterprise_b2b"]
                b2b_tokens = b2b_tokenizer.tokenize(text)
                b2b_ids = b2b_tokenizer.encode(text)
                
                result["enterprise_b2b"] = {
                    "token_count": len(b2b_tokens),
                    "tokens": b2b_tokens[:10],  # First 10 tokens
                    "compression_ratio": len(text) / len(b2b_tokens) if b2b_tokens else 0
                }
            
            # Test with Consumer End User tokenizer
            if "consumer_end_user" in self.tokenizers:
                consumer_tokenizer = self.tokenizers["consumer_end_user"]
                consumer_tokens = consumer_tokenizer.tokenize(text)
                consumer_ids = consumer_tokenizer.encode(text)
                
                result["consumer_end_user"] = {
                    "token_count": len(consumer_tokens),
                    "tokens": consumer_tokens[:10],  # First 10 tokens
                    "compression_ratio": len(text) / len(consumer_tokens) if consumer_tokens else 0
                }
            
            # Compare results
            if result["enterprise_b2b"] and result["consumer_end_user"]:
                b2b_count = result["enterprise_b2b"]["token_count"]
                consumer_count = result["consumer_end_user"]["token_count"]
                
                result["comparison"] = {
                    "token_difference": abs(b2b_count - consumer_count),
                    "b2b_more_efficient": b2b_count < consumer_count,
                    "consumer_more_efficient": consumer_count < b2b_count,
                    "efficiency_difference_percent": abs(b2b_count - consumer_count) / max(b2b_count, consumer_count) * 100
                }
            
            comparison_results.append(result)
            
            # Print comparison
            if result["enterprise_b2b"] and result["consumer_end_user"]:
                b2b_count = result["enterprise_b2b"]["token_count"]
                consumer_count = result["consumer_end_user"]["token_count"]
                print(f"  Enterprise B2B: {b2b_count} tokens")
                print(f"  Consumer: {consumer_count} tokens")
                print(f"  Difference: {abs(b2b_count - consumer_count)} tokens")
        
        # Calculate aggregate statistics
        if comparison_results:
            b2b_avg_tokens = sum(r["enterprise_b2b"].get("token_count", 0) for r in comparison_results) / len(comparison_results)
            consumer_avg_tokens = sum(r["consumer_end_user"].get("token_count", 0) for r in comparison_results) / len(comparison_results)
            
            aggregate_stats = {
                "total_tests": len(comparison_results),
                "average_b2b_tokens": b2b_avg_tokens,
                "average_consumer_tokens": consumer_avg_tokens,
                "b2b_generally_more_efficient": b2b_avg_tokens < consumer_avg_tokens,
                "average_compression_difference": abs(b2b_avg_tokens - consumer_avg_tokens),
                "results": comparison_results
            }
        else:
            aggregate_stats = {"error": "No valid comparisons performed"}
        
        print(f"\nüìä COMPARISON SUMMARY:")
        print(f"   Enterprise B2B avg: {b2b_avg_tokens:.1f} tokens")
        print(f"   Consumer avg: {consumer_avg_tokens:.1f} tokens")
        print(f"   Difference: {abs(b2b_avg_tokens - consumer_avg_tokens):.1f} tokens")
        
        return aggregate_stats
    
    def load_tokenizers(self, enterprise_path: str = None, consumer_path: str = None) -> Dict[str, bool]:
        """Load pre-trained tokenizers from disk."""
        
        results = {"enterprise_b2b": False, "consumer_end_user": False}
        
        # Load Enterprise B2B tokenizer
        enterprise_dir = enterprise_path or self.tokenizer_configs["enterprise_b2b"].output_dir
        if os.path.exists(enterprise_dir):
            try:
                self.tokenizers["enterprise_b2b"] = PreTrainedTokenizerFast.from_pretrained(enterprise_dir)
                results["enterprise_b2b"] = True
                print(f"‚úÖ Enterprise B2B tokenizer loaded from: {enterprise_dir}")
            except Exception as e:
                print(f"‚ùå Failed to load Enterprise B2B tokenizer: {e}")
        
        # Load Consumer End User tokenizer
        consumer_dir = consumer_path or self.tokenizer_configs["consumer_end_user"].output_dir
        if os.path.exists(consumer_dir):
            try:
                self.tokenizers["consumer_end_user"] = PreTrainedTokenizerFast.from_pretrained(consumer_dir)
                results["consumer_end_user"] = True
                print(f"‚úÖ Consumer End User tokenizer loaded from: {consumer_dir}")
            except Exception as e:
                print(f"‚ùå Failed to load Consumer End User tokenizer: {e}")
        
        return results
    
    def get_appropriate_tokenizer(self, text: str, context_hints: List[str] = None) -> str:
        """Determine which tokenizer is most appropriate for given text."""
        
        # Keywords that suggest enterprise B2B context
        b2b_keywords = [
            "batch", "commercial", "industrial", "service", "compliance", "HACCP",
            "cost_percentage", "yield", "equipment", "staff", "procedure", "regulation",
            "supplier", "inventory", "portion_control", "food_safety", "temperature_log",
            "recipe_multiplier", "production", "scaling", "efficiency"
        ]
        
        # Keywords that suggest consumer context
        consumer_keywords = [
            "family", "home", "easy", "quick", "weeknight", "kid_friendly", "budget",
            "pantry", "leftover", "comfort", "weekend", "homemade", "simple", "delicious",
            "favorite", "cozy", "fresh", "seasonal", "healthy", "meal_prep"
        ]
        
        text_lower = text.lower()
        
        # Count keyword matches
        b2b_score = sum(1 for keyword in b2b_keywords if keyword.replace("_", " ") in text_lower)
        consumer_score = sum(1 for keyword in consumer_keywords if keyword.replace("_", " ") in text_lower)
        
        # Consider context hints if provided
        if context_hints:
            context_lower = [hint.lower() for hint in context_hints]
            if any("commercial" in hint or "enterprise" in hint or "b2b" in hint for hint in context_lower):
                b2b_score += 5
            if any("home" in hint or "consumer" in hint or "family" in hint for hint in context_lower):
                consumer_score += 5
        
        # Determine recommendation
        if b2b_score > consumer_score:
            return "enterprise_b2b"
        elif consumer_score > b2b_score:
            return "consumer_end_user"
        else:
            return "either"  # Could use either tokenizer
    
    def send_training_summary_notification(self, results: Dict[str, Any]):
        """Send comprehensive training summary to Discord."""
        
        if not self.discord_webhook:
            return
        
        try:
            # Determine overall success
            successful_tokenizers = [k for k, v in results["results"].items() if v.get("training_success", False)]
            failed_tokenizers = [k for k, v in results["results"].items() if "error" in v]
            
            if len(successful_tokenizers) == 2:
                color = 0x00ff00  # Green - all success
                status_emoji = "üéâ"
                status_text = "ALL TOKENIZERS TRAINED SUCCESSFULLY"
            elif len(successful_tokenizers) == 1:
                color = 0xffaa00  # Orange - partial success
                status_emoji = "‚ö†Ô∏è"
                status_text = "PARTIAL SUCCESS"
            else:
                color = 0xff0000  # Red - all failed
                status_emoji = "‚ùå"
                status_text = "TRAINING FAILED"
            
            embed = {
                "title": f"{status_emoji} Dual Tokenizer Training Complete",
                "description": f"**{status_text}**\nTrained both Enterprise B2B and Consumer End User tokenizers",
                "color": color,
                "timestamp": time.strftime("%Y-%m-%dT%H:%M:%S.000Z"),
                "fields": [
                    {
                        "name": "‚ö° Training Performance",
                        "value": f"üîÑ Mode: {results['training_mode'].title()}\n‚è±Ô∏è Total time: {results['total_training_time']:.1f}s\n‚úÖ Success: {len(successful_tokenizers)}/2",
                        "inline": True
                    }
                ]
            }
            
            # Add results for each tokenizer
            for tokenizer_type, result in results["results"].items():
                if "error" not in result:
                    name = "üè≠ Enterprise B2B" if tokenizer_type == "enterprise_b2b" else "üè† Consumer End User"
                    test_results = result.get("test_results", {})
                    embed["fields"].append({
                        "name": name,
                        "value": f"üî§ Vocab: {result['vocab_size']:,}\nüìÅ Path: `{result['saved_path']}`\nüß™ Tests: {test_results.get('successful_round_trips', 0)}/{test_results.get('total_tests', 0)}",
                        "inline": True
                    })
                else:
                    name = f"‚ùå {tokenizer_type.replace('_', ' ').title()}"
                    embed["fields"].append({
                        "name": name,
                        "value": f"Error: {result['error'][:100]}",
                        "inline": True
                    })
            
            # Add usage guidance
            embed["fields"].append({
                "name": "üéØ Usage Guidance",
                "value": "‚Ä¢ **Enterprise B2B**: Commercial kitchens, large batch cooking, compliance\n‚Ä¢ **Consumer**: Home cooking, family recipes, casual dining",
                "inline": False
            })
            
            payload = {
                "embeds": [embed],
                "username": "Chef Genius Dual Tokenizer Manager"
            }
            
            response = requests.post(self.discord_webhook, json=payload, timeout=10)
            response.raise_for_status()
            
            print("üîî Training summary notification sent successfully!")
            
        except Exception as e:
            print(f"‚ö†Ô∏è Failed to send Discord notification: {e}")
    
    def create_deployment_package(self, output_dir: str = "chef_genius_tokenizers") -> str:
        """Create a deployment package with both tokenizers and usage guide."""
        
        os.makedirs(output_dir, exist_ok=True)
        
        # Copy tokenizer directories
        enterprise_dir = self.tokenizer_configs["enterprise_b2b"].output_dir
        consumer_dir = self.tokenizer_configs["consumer_end_user"].output_dir
        
        if os.path.exists(enterprise_dir):
            import shutil
            shutil.copytree(enterprise_dir, f"{output_dir}/enterprise_b2b", dirs_exist_ok=True)
        
        if os.path.exists(consumer_dir):
            import shutil
            shutil.copytree(consumer_dir, f"{output_dir}/consumer_end_user", dirs_exist_ok=True)
        
        # Create usage guide
        usage_guide = self._create_usage_guide()
        with open(f"{output_dir}/USAGE_GUIDE.md", 'w') as f:
            f.write(usage_guide)
        
        # Create integration examples
        integration_code = self._create_integration_examples()
        with open(f"{output_dir}/integration_examples.py", 'w') as f:
            f.write(integration_code)
        
        # Create deployment config
        deployment_config = {
            "package_created": time.strftime("%Y-%m-%d %H:%M:%S"),
            "tokenizers": {
                "enterprise_b2b": {
                    "path": "enterprise_b2b",
                    "vocab_size": self.tokenizer_configs["enterprise_b2b"].vocab_size,
                    "target_domain": "commercial_food_service"
                },
                "consumer_end_user": {
                    "path": "consumer_end_user", 
                    "vocab_size": self.tokenizer_configs["consumer_end_user"].vocab_size,
                    "target_domain": "home_cooking_consumer"
                }
            },
            "files": ["USAGE_GUIDE.md", "integration_examples.py"]
        }
        
        with open(f"{output_dir}/deployment_config.json", 'w') as f:
            json.dump(deployment_config, f, indent=2)
        
        print(f"üì¶ Deployment package created: {output_dir}")
        return output_dir
    
    def _create_usage_guide(self) -> str:
        """Create comprehensive usage guide for both tokenizers."""
        
        return '''# Chef Genius Dual Tokenizer Usage Guide

## Overview

This package contains two specialized tokenizers for different culinary domains:

### üè≠ Enterprise B2B Tokenizer
- **Target**: Commercial kitchens, food service operations
- **Vocabulary**: 50,000 tokens
- **Specializations**: 
  - Large batch cooking (100+ servings)
  - Commercial equipment and procedures
  - Food safety compliance and HACCP
  - Cost control and yield management
  - Supply chain and inventory management

### üè† Consumer End User Tokenizer  
- **Target**: Home cooks, casual cooking
- **Vocabulary**: 32,000 tokens
- **Specializations**:
  - Family-friendly recipes
  - Home kitchen equipment
  - Quick weeknight meals
  - Budget-conscious cooking
  - Seasonal and occasion-based cooking

## When to Use Which Tokenizer

### Use Enterprise B2B for:
- Commercial recipe scaling
- Food service operations
- Compliance documentation
- Supply chain content
- Staff training materials
- Cost analysis reports

### Use Consumer End User for:
- Home recipe content
- Family meal planning
- Cooking blogs and tutorials
- Casual cooking instructions
- Personal recipe collections

## Loading Tokenizers

```python
from transformers import PreTrainedTokenizerFast

# Load Enterprise B2B tokenizer
enterprise_tokenizer = PreTrainedTokenizerFast.from_pretrained("./enterprise_b2b")

# Load Consumer tokenizer
consumer_tokenizer = PreTrainedTokenizerFast.from_pretrained("./consumer_end_user")
```

## Performance Expectations

- **Enterprise B2B**: Better compression for technical culinary terms
- **Consumer**: Better compression for casual cooking language
- Both tokenizers handle their respective domains ~15-20% more efficiently than generic tokenizers

## Integration Tips

1. **Content Classification**: Use keyword analysis to automatically select appropriate tokenizer
2. **Hybrid Systems**: Use both tokenizers based on user context (B2B vs consumer interface)
3. **Fine-tuning**: Both tokenizers work excellent with FLAN-T5 and similar seq2seq models
'''
    
    def _create_integration_examples(self) -> str:
        """Create integration example code."""
        
        return '''#!/usr/bin/env python3
"""
Chef Genius Tokenizer Integration Examples
"""

from transformers import PreTrainedTokenizerFast
from typing import List

class ChefGeniusTokenizerManager:
    """Example integration for Chef Genius dual tokenizers."""
    
    def __init__(self, enterprise_path: str, consumer_path: str):
        self.enterprise_tokenizer = PreTrainedTokenizerFast.from_pretrained(enterprise_path)
        self.consumer_tokenizer = PreTrainedTokenizerFast.from_pretrained(consumer_path)
    
    def select_tokenizer(self, text: str, context: str = "auto"):
        """Select appropriate tokenizer based on text and context."""
        
        if context == "enterprise" or context == "b2b":
            return self.enterprise_tokenizer
        elif context == "consumer" or context == "home":
            return self.consumer_tokenizer
        elif context == "auto":
            return self.auto_select_tokenizer(text)
        else:
            return self.consumer_tokenizer  # Default to consumer
    
    def auto_select_tokenizer(self, text: str):
        """Automatically select tokenizer based on content analysis."""
        
        # Enterprise keywords
        enterprise_keywords = [
            "batch", "commercial", "HACCP", "compliance", "yield", 
            "cost_percentage", "equipment", "procedure", "production"
        ]
        
        # Consumer keywords  
        consumer_keywords = [
            "family", "home", "easy", "quick", "delicious", "comfort",
            "weeknight", "budget", "homemade", "fresh"
        ]
        
        text_lower = text.lower()
        enterprise_score = sum(1 for kw in enterprise_keywords if kw in text_lower)
        consumer_score = sum(1 for kw in consumer_keywords if kw in text_lower)
        
        return self.enterprise_tokenizer if enterprise_score > consumer_score else self.consumer_tokenizer
    
    def tokenize_recipe(self, recipe_text: str, context: str = "auto"):
        """Tokenize recipe using appropriate tokenizer."""
        
        tokenizer = self.select_tokenizer(recipe_text, context)
        return tokenizer.tokenize(recipe_text)
    
    def encode_for_training(self, texts: List[str], context: str = "auto"):
        """Encode texts for model training."""
        
        encoded_batches = []
        for text in texts:
            tokenizer = self.select_tokenizer(text, context)
            encoded = tokenizer.encode(text, truncation=True, max_length=512)
            encoded_batches.append(encoded)
        
        return encoded_batches

# Example usage
if __name__ == "__main__":
    
    # Initialize manager
    manager = ChefGeniusTokenizerManager(
        enterprise_path="./enterprise_b2b",
        consumer_path="./consumer_end_user"
    )
    
    # Example texts
    enterprise_text = "Set combi_oven to 325¬∞F and prepare 100 servings using HACCP protocol."
    consumer_text = "This easy 30-minute weeknight dinner is perfect for busy families!"
    
    # Tokenize with auto-selection
    enterprise_tokens = manager.tokenize_recipe(enterprise_text)
    consumer_tokens = manager.tokenize_recipe(consumer_text)
    
    print(f"Enterprise tokens: {len(enterprise_tokens)}")
    print(f"Consumer tokens: {len(consumer_tokens)}")
'''

def main():
    """Main function for dual tokenizer management."""
    
    import argparse
    
    parser = argparse.ArgumentParser(description='Dual Tokenizer Manager for Chef Genius')
    parser.add_argument('--train', action='store_true', help='Train both tokenizers')
    parser.add_argument('--parallel', action='store_true', help='Train tokenizers in parallel')
    parser.add_argument('--compare', action='store_true', help='Compare tokenizers')
    parser.add_argument('--load', action='store_true', help='Load existing tokenizers')
    parser.add_argument('--deploy', action='store_true', help='Create deployment package')
    parser.add_argument('--discord-webhook', type=str, 
                       default='https://discord.com/api/webhooks/1386109570283343953/uGkhj9dpuCg09SbKzZ0Tx2evugJrchQv-nrq3w0r_xi3w8si-XBpQJuxq_p_bcQlhB9W',
                       help='Discord webhook for notifications')
    
    args = parser.parse_args()
    
    print("üî• CHEF GENIUS DUAL TOKENIZER MANAGER")
    print("Enterprise B2B + Consumer End User Tokenizers")
    print("=" * 80)
    
    # Create manager
    manager = DualTokenizerManager(discord_webhook=args.discord_webhook)
    
    if args.train:
        # Train tokenizers
        results = manager.train_all_tokenizers(parallel=args.parallel)
        print(f"\nüéâ Training Complete!")
        print(f"   Mode: {'Parallel' if args.parallel else 'Sequential'}")
        print(f"   Time: {results['total_training_time']:.1f}s")
        print(f"   Success: {results['tokenizers_trained']}/2")
    
    if args.load:
        # Load existing tokenizers
        load_results = manager.load_tokenizers()
        loaded_count = sum(load_results.values())
        print(f"\nüìÅ Loaded {loaded_count}/2 tokenizers")
    
    if args.compare:
        # Compare tokenizers
        if not manager.tokenizers:
            manager.load_tokenizers()
        
        if manager.tokenizers:
            comparison = manager.compare_tokenizers()
            print(f"\nüîç Comparison complete!")
        else:
            print("‚ö†Ô∏è No tokenizers available for comparison")
    
    if args.deploy:
        # Create deployment package
        if not manager.tokenizers:
            manager.load_tokenizers()
        
        deploy_path = manager.create_deployment_package()
        print(f"\nüì¶ Deployment package ready: {deploy_path}")

if __name__ == "__main__":
    main()