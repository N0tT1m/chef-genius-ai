# ğŸ³ Docker Compose Quick Start - RTX 5090 Optimized

## âœ… Ready to Train with Docker Compose

Your `docker-compose.training.yml` is now fully optimized for **RTX 5090 + Ryzen 3900X + 48GB RAM**.

---

## ğŸš€ Start Training (Primary Method)

```bash
# Build and start training
docker-compose -f docker-compose.training.yml up --build
```

That's it! The container will automatically:
- âœ… Use batch_size=48 (optimized for RTX 5090)
- âœ… Use 16 dataloader workers (Ryzen 3900X)
- âœ… Train FLAN-T5-Large with 2.4M+ recipes
- âœ… Send Discord alerts to your webhook
- âœ… Send SMS alerts to +18125841533
- âœ… Log to W&B dashboard
- âœ… Save checkpoints every 1000 steps
- âœ… Complete 5 epochs in ~6-8 hours

---

## ğŸ“Š Monitor Training

### View Live Logs
```bash
# Follow logs in real-time
docker-compose -f docker-compose.training.yml logs -f chef-genius-training
```

### Check GPU Usage
```bash
# In another terminal
watch -n 1 nvidia-smi
```

Expected output:
```
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 535.xx.xx              Driver Version: 535.xx.xx    CUDA: 12.6 |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Utilization | Memory-Usage         | GPU-Temp  Perf  Pwr  |
|===============================+======================+======================|
|   0  RTX 5090        95-100% | 20-24GB / 32GB       | 70-80Â°C   P0    450W |
+-------------------------------+----------------------+----------------------+
```

### View TensorBoard (Optional)
```bash
# TensorBoard is automatically started
# Open browser to: http://localhost:6006
```

---

## ğŸ¯ Optimized Configuration Summary

### Docker Compose Settings
```yaml
shm_size: 16gb          # Shared memory for large batches
mem_limit: 40g          # Use 40GB of your 48GB RAM
VRAM: 32GB              # RTX 5090
```

### Training Settings (from Dockerfile CMD)
```bash
--batch-size 48                    # 3x larger than 4090 config
--gradient-accumulation-steps 1    # No accumulation needed
--dataloader-num-workers 16        # Use 66% of Ryzen cores
--epochs 5                         # Full training run
```

### Memory Allocation
```
Model (bfloat16):      ~1.5 GB
Optimizer (AdamW):     ~3.0 GB
Gradients:             ~1.5 GB
Activations (batch=48): 12-14 GB
Buffer:                ~2 GB
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total GPU Memory:      20-22 GB / 32 GB (69% utilization)
System RAM:            18-22 GB / 40 GB (Docker limit)
```

---

## â±ï¸ Training Timeline

| Phase | Duration | Notes |
|-------|----------|-------|
| **Docker Build** | 5-10 min | First time only (cached after) |
| **Data Loading** | 2-3 min | Rust dataloader warmup |
| **Epoch 1** | ~1.5 hours | Model warmup + compilation |
| **Epochs 2-5** | ~5 hours | Steady state training |
| **Total** | **~6.5-8 hours** | For 5 epochs on 2.4M samples |

---

## ğŸ›ï¸ Control Commands

### Start Training (Foreground)
```bash
docker-compose -f docker-compose.training.yml up
```

### Start Training (Background)
```bash
docker-compose -f docker-compose.training.yml up -d
```

### Stop Training
```bash
docker-compose -f docker-compose.training.yml down
```

### Rebuild Container (After Code Changes)
```bash
docker-compose -f docker-compose.training.yml build --no-cache
docker-compose -f docker-compose.training.yml up
```

### View Container Status
```bash
docker-compose -f docker-compose.training.yml ps
```

### Execute Command in Running Container
```bash
docker-compose -f docker-compose.training.yml exec chef-genius-training bash
```

---

## ğŸ“± Notifications

### Discord Alerts
You'll receive Discord notifications for:
- âœ… Training started
- ğŸ“Š Progress updates (every epoch)
- âš ï¸ Data pipeline bottlenecks
- ğŸ’¥ Crashes with stack traces
- âœ… Training completed

### SMS Alerts
You'll receive SMS notifications for:
- ğŸš€ Training started
- ğŸ“Š Progress updates
- âŒ Critical errors
- âœ… Training completed

Both are configured automatically via environment variables in `docker-compose.training.yml`.

---

## ğŸ“‚ Output Locations

### Model Checkpoints
```
./models/chef-genius-flan-t5-large-6m-recipes/
â”œâ”€â”€ checkpoint-1000/
â”œâ”€â”€ checkpoint-2000/
â”œâ”€â”€ checkpoint-3000/
â””â”€â”€ [final model files]
```

### Training Logs
```
./logs/
â””â”€â”€ training_*.log
```

### W&B Dashboard
- Project: `chef-genius-optimized`
- Run name: `optimized-training-YYYYMMDD-HHMMSS`
- URL: Printed in console on startup

---

## ğŸ”§ Environment Variables (Optional Overrides)

Create `.env.training` file to override defaults:

```bash
# .env.training
WANDB_API_KEY=your_wandb_key_here
DISCORD_WEBHOOK=your_webhook_url_here
ALERT_PHONE=+1234567890
```

Then the compose file will automatically load it.

---

## ğŸ› Troubleshooting

### OOM Error (Out of Memory)
```bash
# Reduce batch size
# Edit Dockerfile.training line 217:
--batch-size 32  # Instead of 48

# Rebuild
docker-compose -f docker-compose.training.yml build --no-cache
```

### Training Too Slow (< 50 samples/sec)
```bash
# Check if other GPU processes are running
nvidia-smi

# Kill competing processes
# Then restart training
```

### Container Won't Start
```bash
# Check Docker has GPU access
docker run --rm --gpus all nvidia/cuda:12.6.2-base-ubuntu22.04 nvidia-smi

# If that fails, reinstall nvidia-container-toolkit
```

### Checkpoint Not Loading
```bash
# Check checkpoint exists
ls -la ./models/recipe_generation_flan-t5-large/checkpoint-1000/

# If missing, remove --resume-from-checkpoint from Dockerfile CMD
```

---

## ğŸ“Š Expected Performance Metrics

During training, you should see:

```
ğŸš€ Starting epoch 1/5
Step 1,000 | Loss: 2.3456
âœ… Saved checkpoint: /workspace/models/.../checkpoint-1000

Step 2,000 | Loss: 1.8234
âœ… Saved checkpoint: /workspace/models/.../checkpoint-2000

ğŸ“Š Training Progress
   Progress: 20.0% (1/5)
   Loss: 1.7123
   Learning Rate: 4.5e-04
   Data Speed: 105.3 samples/sec
```

**Key Metrics:**
- Samples/sec: **96-120** âœ…
- GPU Utilization: **95-100%** âœ…
- VRAM Usage: **20-24GB / 32GB** âœ…
- Loss: Should decrease steadily

---

## ğŸŠ You're All Set!

Just run:
```bash
docker-compose -f docker-compose.training.yml up --build
```

And your RTX 5090 will train **3-4x faster** than the old config! ğŸš€

---

## ğŸ“ Support

- Discord alerts: Automatic
- SMS alerts: Automatic
- W&B dashboard: Check console for URL
- Logs: `docker-compose logs -f`

**Happy training!** ğŸ³
